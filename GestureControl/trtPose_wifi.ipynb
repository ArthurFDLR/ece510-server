{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MQTT client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paho.mqtt.client as mqtt\n",
    "\n",
    "client = mqtt.Client()\n",
    "\n",
    "client.connect(\"192.168.0.151\", 1883, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc73d08d-19f6-49aa-a85a-2d2d52c581da",
   "metadata": {},
   "source": [
    "# Import pose estimation model\n",
    "\n",
    "## Define output format\n",
    "\n",
    "Let's load the JSON file which describes the human pose task.  This is in COCO format, it is the category descriptor pulled from the annotations file.  We modify the COCO category slightly, to add a neck keypoint.  We will use this task description JSON to create a topology tensor, which is an intermediate data structure that describes the part linkages, as well as which channels in the part affinity field each linkage corresponds to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a34bead-4989-4e7b-8c7f-2f09b611375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + \"/configs/\" # Specify MatplotLib config folder\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "# Requiere https://github.com/NVIDIA-AI-IOT/trt_pose\n",
    "import trt_pose.coco\n",
    "from trt_pose.draw_objects import DrawObjects\n",
    "from trt_pose.parse_objects import ParseObjects\n",
    "\n",
    "with open('human_pose.json', 'r') as f:\n",
    "    human_pose = json.load(f)\n",
    "\n",
    "topology = trt_pose.coco.coco_category_to_topology(human_pose)\n",
    "\n",
    "parse_objects = ParseObjects(topology)\n",
    "draw_objects = DrawObjects(topology)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c31468-df1c-43c2-bd59-40c50401e2e0",
   "metadata": {},
   "source": [
    "## Import TensorRT optimized model\n",
    "\n",
    "Next, we'll load our model. It has been optimized using another Notebook and saved so that we do not need to perform optimization again, we can just load the model. Please note that TensorRT has device specific optimizations, so you can only use an optimized model on similar platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc10e8eb-786c-438e-810f-77f069f5ff0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Requiere https://github.com/NVIDIA-AI-IOT/torch2trt\n",
    "from torch2trt import TRTModule\n",
    "\n",
    "OPTIMIZED_MODEL = 'resnet18_baseline_att_224x224_A_epoch_249_trt.pth'\n",
    "\n",
    "model_trt = TRTModule()\n",
    "model_trt.load_state_dict(torch.load(OPTIMIZED_MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbed1cb-6960-4223-bb9e-fe2671f69e8d",
   "metadata": {},
   "source": [
    "# Define video-processing pipeline\n",
    "\n",
    "## Pre-process image for TRT_Pose\n",
    "\n",
    "Next, let's define a function that will preprocess the image, which is originally in BGR8 / HWC format. It is formated to the default Torch format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3b4edef-b585-4a5e-a6a7-c150e3a026ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import PIL.Image\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda()\n",
    "device = torch.device('cuda')\n",
    "\n",
    "def preprocess(image):\n",
    "    global device\n",
    "    device = torch.device('cuda')\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device)\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60b9d20-30cb-4f17-a528-e87b1678a64e",
   "metadata": {},
   "source": [
    "## Access video feed\n",
    "\n",
    "Access images streamed by a WiFi camera on the local network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79780417-b089-420f-9a50-8473a177a4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60bd5c37f1424060838f64fe7c09c864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpeg')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WIDTH, HEIGHT = 224, 224 # Imposed by the model\n",
    "\n",
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "import urllib.request\n",
    "\n",
    "image_w = ipywidgets.Image(format='jpeg')\n",
    "\n",
    "display(image_w)\n",
    "\n",
    "url_esp32 = 'http://192.168.0.162/capture'\n",
    "def fetch_image(url):\n",
    "    imgResp = urllib.request.urlopen(url)\n",
    "    imgNp = np.array(bytearray(imgResp.read()),dtype=np.uint8)\n",
    "    img = cv2.imdecode(imgNp,-1)\n",
    "    return cv2.resize(img, (WIDTH, HEIGHT), interpolation = cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get keypoints from the peaks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoint(objects, index, peaks):\n",
    "    kpoint = []\n",
    "    human = objects[0][index]\n",
    "    C = human.shape[0]\n",
    "    for j in range(C):\n",
    "        k = int(human[j])\n",
    "        if k >= 0:\n",
    "            peak = peaks[0][j][k]   # peak[1]:width, peak[0]:height\n",
    "            peak = (j, float(peak[0]), float(peak[1]))\n",
    "            kpoint.append(peak)\n",
    "            print('index:%d : success [%5.3f, %5.3f]'%(j, peak[1], peak[2]) )\n",
    "        else:    \n",
    "            peak = (j, None, None)\n",
    "            kpoint.append(peak)\n",
    "            print('index:%d : None'%(j) )\n",
    "    return kpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Detect if one hand is raised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isRaisingHand(poseKeypoints):\n",
    "    # TODO : transform counts, objects and peaks into keypoints, Also find the right keypoints\n",
    "        raisingLeft = False\n",
    "        raisingRight = False\n",
    "        rightHand_idx, rightHand_y, rightHand_x = poseKeypoints[10]\n",
    "        leftHand_idx, leftHand_y, leftHand_x = poseKeypoints[9]\n",
    "        rightShoulder_idx, rightShoulder_y, rightShoulder_x = poseKeypoints[6]\n",
    "        leftShoulder_idx, leftShoulder_y, leftShoulder_x = poseKeypoints[5]\n",
    "        try:\n",
    "            shoulderSlope = (rightShoulder_y - leftShoulder_y) / (\n",
    "                rightShoulder_x - leftShoulder_x\n",
    "            )\n",
    "        except:\n",
    "            shoulderSlope = 0.0\n",
    "        shoulderOri = rightShoulder_y - shoulderSlope * rightShoulder_x\n",
    "\n",
    "        if leftHand_a > 0.1:\n",
    "            raisingLeft = leftHand_y > (\n",
    "                shoulderSlope * leftHand_x + shoulderOri\n",
    "            )  # y axis oriented from top to down in images\n",
    "            raisingLeft = (\n",
    "                raisingLeft and leftHand_y > poseKeypoints[6, 1]\n",
    "            )  # Check if hand above elbow\n",
    "        else:\n",
    "            raisingLeft= False\n",
    "\n",
    "        if rightHand_a > 0.1:\n",
    "            raisingRight = rightHand_y > (shoulderSlope * rightHand_x + shoulderOri)\n",
    "            raisingRight = raisingRight and rightHand_y > poseKeypoints[3, 1]\n",
    "        else:\n",
    "            raisingRight = False\n",
    "        return raisingRight or raisingLeft"
   ]
  },
  {
   "source": [
    "## Processing loop\n",
    "\n",
    "The *execute()* function contains the whole analysis process: \n",
    "- Read image\n",
    "- Pre-process to Torch format\n",
    "- Infere key-points\n",
    "- Draw skeleton on the input image\n",
    "- Update in output window."
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7445e59f-e997-423a-88a8-0c650c5685c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video processing stopped\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while True:\n",
    "        image = fetch_image(url_esp32)\n",
    "        data = preprocess(image)\n",
    "        cmap, paf = model_trt(data)\n",
    "        cmap, paf = cmap.detach().cpu(), paf.detach().cpu()\n",
    "        counts, objects, peaks = parse_objects(cmap, paf) #, cmap_threshold=0.15, link_threshold=0.15)\n",
    "        for i in range(counts[0]):\n",
    "            print(\"Human index:%d \"%( i ))\n",
    "            keypoints = get_keypoint(objects, i, peaks)\n",
    "            if isRaisingHand(keypoints):\n",
    "                client.publish(\"lamp\", \"2\")\n",
    "        draw_objects(image, counts, objects, peaks)\n",
    "        image_w.value = bytes(cv2.imencode('.jpg', image[:, ::-1, :])[1])\n",
    "except KeyboardInterrupt:\n",
    "    print('Video processing stopped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080335d9-e7ae-455a-93ff-aa7b5787d3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}