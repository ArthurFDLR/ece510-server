{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc73d08d-19f6-49aa-a85a-2d2d52c581da",
   "metadata": {},
   "source": [
    "# Import pose estimation model\n",
    "\n",
    "## Define output format\n",
    "\n",
    "Let's load the JSON file which describes the human pose task.  This is in COCO format, it is the category descriptor pulled from the annotations file.  We modify the COCO category slightly, to add a neck keypoint.  We will use this task description JSON to create a topology tensor, which is an intermediate data structure that describes the part linkages, as well as which channels in the part affinity field each linkage corresponds to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a34bead-4989-4e7b-8c7f-2f09b611375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + \"/configs/\" # Specify MatplotLib config folder\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "# Requiere https://github.com/NVIDIA-AI-IOT/trt_pose\n",
    "import trt_pose.coco\n",
    "from trt_pose.draw_objects import DrawObjects\n",
    "from trt_pose.parse_objects import ParseObjects\n",
    "\n",
    "with open('human_pose.json', 'r') as f:\n",
    "    human_pose = json.load(f)\n",
    "\n",
    "topology = trt_pose.coco.coco_category_to_topology(human_pose)\n",
    "\n",
    "parse_objects = ParseObjects(topology)\n",
    "draw_objects = DrawObjects(topology)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c31468-df1c-43c2-bd59-40c50401e2e0",
   "metadata": {},
   "source": [
    "## Import TensorRT optimized model\n",
    "\n",
    "Next, we'll load our model. It has been optimized using another Notebook and saved so that we do not need to perform optimization again, we can just load the model. Please note that TensorRT has device specific optimizations, so you can only use an optimized model on similar platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc10e8eb-786c-438e-810f-77f069f5ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Requiere https://github.com/NVIDIA-AI-IOT/torch2trt\n",
    "from torch2trt import TRTModule\n",
    "\n",
    "OPTIMIZED_MODEL = 'resnet18_baseline_att_224x224_A_epoch_249_trt.pth'\n",
    "\n",
    "model_trt = TRTModule()\n",
    "model_trt.load_state_dict(torch.load(OPTIMIZED_MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbed1cb-6960-4223-bb9e-fe2671f69e8d",
   "metadata": {},
   "source": [
    "# Define video-processing pipeline\n",
    "\n",
    "## Pre-process image for TRT_Pose\n",
    "\n",
    "Next, let's define a function that will preprocess the image, which is originally in BGR8 / HWC format. It is formated to the default Torch format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b4edef-b585-4a5e-a6a7-c150e3a026ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import PIL.Image\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda()\n",
    "device = torch.device('cuda')\n",
    "\n",
    "def preprocess(image):\n",
    "    global device\n",
    "    device = torch.device('cuda')\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device)\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60b9d20-30cb-4f17-a528-e87b1678a64e",
   "metadata": {},
   "source": [
    "## Access video feed\n",
    "\n",
    "Access images streamed by a WiFi camera on the local network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79780417-b089-420f-9a50-8473a177a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH, HEIGHT = 224, 224 # Imposed by the model\n",
    "\n",
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "import urllib.request\n",
    "\n",
    "image_w = ipywidgets.Image(format='jpeg')\n",
    "\n",
    "display(image_w)\n",
    "\n",
    "url_esp32 = 'http://192.168.0.163/capture'\n",
    "def fetch_image(url):\n",
    "    imgResp = urllib.request.urlopen(url)\n",
    "    imgNp = np.array(bytearray(imgResp.read()),dtype=np.uint8)\n",
    "    img = cv2.imdecode(imgNp,-1)\n",
    "    return cv2.resize(img, (WIDTH, HEIGHT), interpolation = cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4668cdbd-3cf0-4b83-9549-bf93ba2fd8d8",
   "metadata": {},
   "source": [
    "## Get keypoints with TRT-Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481c71a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints(counts, objects, peak, indexBody=0):\n",
    "    #if indexBody<counts[0]:\n",
    "    #    return None\n",
    "    kpoint = {}\n",
    "    human = objects[0][indexBody]\n",
    "    C = human.shape[0]\n",
    "    for j in range(C):\n",
    "        k = int(human[j])\n",
    "        if k >= 0:\n",
    "            peak = peaks[0][j][k]   # peak[1]:width, peak[0]:height\n",
    "            kpoint[j] = [float(peak[1]),float(peak[0])]\n",
    "            #print('indexBody:%d : success [%5.3f, %5.3f]'%(j, peak[1], peak[2]) )\n",
    "        else:\n",
    "        \n",
    "            kpoint[j] = [None, None]\n",
    "            #print('indexBody:%d : None'%(j) )\n",
    "    return kpoint\n",
    "\n",
    "def get_cmap_paf(image):\n",
    "        data = preprocess(image)\n",
    "        cmap, paf = model_trt(data)\n",
    "        cmap, paf = cmap.detach().cpu(), paf.detach().cpu()\n",
    "        return cmap, paf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d49692-9506-45d0-9389-77a79e47c9fc",
   "metadata": {},
   "source": [
    "## Keypoints analysis\n",
    "\n",
    "### Detect if left hand is raised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88142777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isRaisingHand(poseKeypoints):        \n",
    "        #rightHand_idx, rightHand_y, rightHand_x = poseKeypoints[9]\n",
    "        leftHand_x, leftHand_y = poseKeypoints[10]\n",
    "        rightShoulder_x, rightShoulder_y = poseKeypoints[6]\n",
    "        leftShoulder_x, leftShoulder_y = poseKeypoints[5]\n",
    "        \n",
    "        if rightShoulder_y and leftShoulder_y and leftHand_y:\n",
    "            #print(rightShoulder_y, leftShoulder_y, leftHand_y)\n",
    "            shoulderSlope = (rightShoulder_y - leftShoulder_y) / (\n",
    "                rightShoulder_x - leftShoulder_x\n",
    "            )\n",
    "            shoulderOri = rightShoulder_y - shoulderSlope * rightShoulder_x\n",
    "            raisingLeft = leftHand_y < (shoulderSlope * leftHand_x + shoulderOri)\n",
    "            return raisingLeft\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f886e258-90ca-4fae-befd-8684da884088",
   "metadata": {},
   "source": [
    "## MQTT connection with IoT hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d494cfe4-6f6b-438d-9722-5e89e1da4c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paho.mqtt.client as mqtt\n",
    "client = mqtt.Client()\n",
    "client.connect('192.168.0.151', 1883, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d5c2ad-6768-403a-bf73-0c09982af1bf",
   "metadata": {},
   "source": [
    "# Main Processing loop\n",
    "\n",
    "- Read image\n",
    "- Infere key-points\n",
    "- Detect raised left hands\n",
    "- Send MQTT update\n",
    "- Draw skeleton on the input image\n",
    "- Update in output window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7445e59f-e997-423a-88a8-0c650c5685c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    client.publish(\"devices/pose-estimation/jetson-nano-1/status\", \"online\")\n",
    "    while True:\n",
    "        # Video processing\n",
    "        image = fetch_image(url_esp32)\n",
    "        cmap, paf = get_cmap_paf(image)\n",
    "        counts, objects, peaks = parse_objects(cmap, paf)\n",
    "        keypoints = get_keypoints(counts, objects, peaks)\n",
    "        \n",
    "        # Keypoints analysis\n",
    "        label_pose = \"No_Pose\"\n",
    "        if type(keypoints) != type(None) and isRaisingHand(keypoints):\n",
    "            label_pose = \"RaisedHand\"\n",
    "            \n",
    "        # Send MQTT update\n",
    "        pose_info = {\"PoseLabel\" : label_pose, \"Attributes\" : {\"Keypoints\" : keypoints}}\n",
    "        pose_info_json = json.dumps(pose_info)\n",
    "        client.publish(\"devices/pose-estimation/jetson-nano-1/postures-info\", pose_info_json)\n",
    "        \n",
    "        # Display image locally\n",
    "        draw_objects(image, counts, objects, peaks)\n",
    "        image_w.value = bytes(cv2.imencode('.jpg', image[:, ::-1, :])[1])\n",
    "except KeyboardInterrupt:\n",
    "    client.publish(\"devices/pose-estimation/jetson-nano-1/status\", \"offline\")\n",
    "    print('Video processing stopped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080335d9-e7ae-455a-93ff-aa7b5787d3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
