{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b596e107-379a-48e7-becb-9822c280d205",
   "metadata": {},
   "source": [
    "# MQTT client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b88764cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import paho.mqtt.client as mqtt\n",
    "\n",
    "client = mqtt.Client()\n",
    "\n",
    "client.connect('192.168.0.173', 1883, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc73d08d-19f6-49aa-a85a-2d2d52c581da",
   "metadata": {},
   "source": [
    "# Import pose estimation model\n",
    "\n",
    "## Define output format\n",
    "\n",
    "Let's load the JSON file which describes the human pose task.  This is in COCO format, it is the category descriptor pulled from the annotations file.  We modify the COCO category slightly, to add a neck keypoint.  We will use this task description JSON to create a topology tensor, which is an intermediate data structure that describes the part linkages, as well as which channels in the part affinity field each linkage corresponds to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a34bead-4989-4e7b-8c7f-2f09b611375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + \"/configs/\" # Specify MatplotLib config folder\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "# Requiere https://github.com/NVIDIA-AI-IOT/trt_pose\n",
    "import trt_pose.coco\n",
    "from trt_pose.draw_objects import DrawObjects\n",
    "from trt_pose.parse_objects import ParseObjects\n",
    "\n",
    "with open('human_pose.json', 'r') as f:\n",
    "    human_pose = json.load(f)\n",
    "\n",
    "topology = trt_pose.coco.coco_category_to_topology(human_pose)\n",
    "\n",
    "parse_objects = ParseObjects(topology)\n",
    "draw_objects = DrawObjects(topology)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c31468-df1c-43c2-bd59-40c50401e2e0",
   "metadata": {},
   "source": [
    "## Import TensorRT optimized model\n",
    "\n",
    "Next, we'll load our model. It has been optimized using another Notebook and saved so that we do not need to perform optimization again, we can just load the model. Please note that TensorRT has device specific optimizations, so you can only use an optimized model on similar platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc10e8eb-786c-438e-810f-77f069f5ff0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Requiere https://github.com/NVIDIA-AI-IOT/torch2trt\n",
    "from torch2trt import TRTModule\n",
    "\n",
    "OPTIMIZED_MODEL = 'resnet18_baseline_att_224x224_A_epoch_249_trt.pth'\n",
    "\n",
    "model_trt = TRTModule()\n",
    "model_trt.load_state_dict(torch.load(OPTIMIZED_MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbed1cb-6960-4223-bb9e-fe2671f69e8d",
   "metadata": {},
   "source": [
    "# Define video-processing pipeline\n",
    "\n",
    "## Pre-process image for TRT_Pose\n",
    "\n",
    "Next, let's define a function that will preprocess the image, which is originally in BGR8 / HWC format. It is formated to the default Torch format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3b4edef-b585-4a5e-a6a7-c150e3a026ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import PIL.Image\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda()\n",
    "device = torch.device('cuda')\n",
    "\n",
    "def preprocess(image):\n",
    "    global device\n",
    "    device = torch.device('cuda')\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device)\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60b9d20-30cb-4f17-a528-e87b1678a64e",
   "metadata": {},
   "source": [
    "## Access video feed\n",
    "\n",
    "Access images streamed by a WiFi camera on the local network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79780417-b089-420f-9a50-8473a177a4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e615007e21784ccaab530dfa729e38cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpeg')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WIDTH, HEIGHT = 224, 224 # Imposed by the model\n",
    "\n",
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "import urllib.request\n",
    "\n",
    "image_w = ipywidgets.Image(format='jpeg')\n",
    "\n",
    "display(image_w)\n",
    "\n",
    "url_esp32 = 'http://192.168.0.162/capture'\n",
    "def fetch_image(url):\n",
    "    imgResp = urllib.request.urlopen(url)\n",
    "    imgNp = np.array(bytearray(imgResp.read()),dtype=np.uint8)\n",
    "    img = cv2.imdecode(imgNp,-1)\n",
    "    return cv2.resize(img, (WIDTH, HEIGHT), interpolation = cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4668cdbd-3cf0-4b83-9549-bf93ba2fd8d8",
   "metadata": {},
   "source": [
    "## Get keypoints from the peaks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "481c71a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoint(objects, index, peaks):\n",
    "    kpoint = []\n",
    "    human = objects[0][index]\n",
    "    C = human.shape[0]\n",
    "    for j in range(C):\n",
    "        k = int(human[j])\n",
    "        if k >= 0:\n",
    "            peak = peaks[0][j][k]   # peak[1]:width, peak[0]:height\n",
    "            peak = (j, float(peak[0]), float(peak[1]))\n",
    "            kpoint.append(peak)\n",
    "            #print('index:%d : success [%5.3f, %5.3f]'%(j, peak[1], peak[2]) )\n",
    "        else:\n",
    "            peak = (j, None, None)\n",
    "            kpoint.append(peak)\n",
    "            #print('index:%d : None'%(j) )\n",
    "    return kpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d49692-9506-45d0-9389-77a79e47c9fc",
   "metadata": {},
   "source": [
    "## Detect if one hand is raised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88142777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isRaisingHand(poseKeypoints):        \n",
    "        rightHand_idx, rightHand_y, rightHand_x = poseKeypoints[9]\n",
    "        leftHand_idx, leftHand_y, leftHand_x = poseKeypoints[10]\n",
    "        rightShoulder_idx, rightShoulder_y, rightShoulder_x = poseKeypoints[6]\n",
    "        leftShoulder_idx, leftShoulder_y, leftShoulder_x = poseKeypoints[5]\n",
    "        \n",
    "        if rightShoulder_y and leftShoulder_y and leftHand_y:\n",
    "            print(rightShoulder_y, leftShoulder_y, leftHand_y)\n",
    "            shoulderSlope = (rightShoulder_y - leftShoulder_y) / (\n",
    "                rightShoulder_x - leftShoulder_x\n",
    "            )\n",
    "            shoulderOri = rightShoulder_y - shoulderSlope * rightShoulder_x\n",
    "            raisingLeft = leftHand_y < (shoulderSlope * leftHand_x + shoulderOri)\n",
    "            return raisingLeft\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d5c2ad-6768-403a-bf73-0c09982af1bf",
   "metadata": {},
   "source": [
    "## Processing loop\n",
    "\n",
    "The *execute()* function contains the whole analysis process: \n",
    "- Read image\n",
    "- Pre-process to Torch format\n",
    "- Infere key-points\n",
    "- Draw skeleton on the input image\n",
    "- Update in output window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7445e59f-e997-423a-88a8-0c650c5685c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6638293862342834 0.6686788201332092 0.7234136462211609\n",
      "0.5800638198852539 0.6216279864311218 0.44287824630737305\n",
      "Raised\n",
      "0.5755125880241394 0.5949109196662903 0.46541109681129456\n",
      "Raised\n",
      "0.5491074919700623 0.6145654320716858 0.5089828372001648\n",
      "0.5462464094161987 0.6149406433105469 0.5093923807144165\n",
      "0.5255587697029114 0.5854342579841614 0.5381296873092651\n",
      "0.5270360112190247 0.5270677208900452 0.8666273355484009\n",
      "0.5339232683181763 0.5439188480377197 0.9788433313369751\n",
      "0.5334664583206177 0.5327016711235046 0.9769620895385742\n",
      "0.5240234136581421 0.5787531733512878 0.43668097257614136\n",
      "Raised\n",
      "0.5262635946273804 0.5842646360397339 0.4211902320384979\n",
      "Raised\n",
      "0.5251557230949402 0.5785633325576782 0.629741370677948\n",
      "0.5300996899604797 0.5762194395065308 0.5083531141281128\n",
      "0.5550853610038757 0.59831303358078 0.7438503503799438\n",
      "Video processing stopped\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while True:\n",
    "        image = fetch_image(url_esp32)\n",
    "        data = preprocess(image)\n",
    "        cmap, paf = model_trt(data)\n",
    "        cmap, paf = cmap.detach().cpu(), paf.detach().cpu()\n",
    "        counts, objects, peaks = parse_objects(cmap, paf) #, cmap_threshold=0.15, link_threshold=0.15)\n",
    "        for i in range(counts[0]):\n",
    "            keypoints = get_keypoint(objects, i, peaks)\n",
    "            if isRaisingHand(keypoints):\n",
    "                client.publish(\"lamp\", \"2\")\n",
    "                print('Raised')\n",
    "        draw_objects(image, counts, objects, peaks)\n",
    "        image_w.value = bytes(cv2.imencode('.jpg', image[:, ::-1, :])[1])\n",
    "except KeyboardInterrupt:\n",
    "    print('Video processing stopped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080335d9-e7ae-455a-93ff-aa7b5787d3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
